{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Có nhiều điều mình không thể hiểu hết về cách đạo hàm của Gradient Descent. <br>\n",
    "Có những bài đạo hàm phía trái, hoặc đạo hàm phía phải cho kết quả. <br>\n",
    "Hoặc là sử dụng Numerical Descent lại cho ra cực trị tối ưu - mặc dù không sử dụng momentum...(Mình có code bên dưới) <br>\n",
    "Bàn luận toán với mình về Gradient Descent <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's many things i can't understand all about how Gradient Descent work. <br>\n",
    "Some gradient by left, some gradient by right, or using Numerical Descent WITHOUT momentum give us optimal value... (I had code below) <br>\n",
    "You can help me to improve math in Gradient Descent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRADIENT DESCENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.rand(1000, 1) # random range(0, 1) with shape (1000, 1)\n",
    "y = X ** 2 + 10 * np.sin(X) + .2 * np.random.randn(1000, 1)\n",
    "\n",
    "# f(x) = x * w^T (x.shape = (N, feature), w^T.shape = (feature, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRADIENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(w: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Hàm mục tiêu để tìm minimum point\n",
    "    \"\"\"\n",
    "    return w ** 2 + 10 * np.sin(w)\n",
    "\n",
    "def grad(w: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Đạo hàm bên phải hàm cost \n",
    "    \"\"\"\n",
    "    w_grad = np.zeros_like(w)\n",
    "    epsilon = 1e-3\n",
    "    for i in range(w.shape[0]):\n",
    "        for j in range(w.shape[1]):\n",
    "            w_right = w.copy()\n",
    "            w_right[i, j] += epsilon\n",
    "            w_grad[i, j] = (cost(w_right) - cost(w)) / epsilon\n",
    "    return w_grad\n",
    "\n",
    "def grad_left(w: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Đạo hàm bên trái hàm cost \n",
    "    \"\"\"\n",
    "    w_grad = np.zeros_like(w)\n",
    "    epsilon = 1e-3\n",
    "    for i in range(w.shape[0]):\n",
    "        for j in range(w.shape[1]):\n",
    "            w_left = w.copy()\n",
    "            w_left[i, j] -= epsilon\n",
    "            w_grad[i, j] = (cost(w) - cost(w_left)) / epsilon\n",
    "    return w_grad\n",
    "\n",
    "def numerical_grad(w: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    w(right) - w(left) / 2 * epsilon\n",
    "    Đạo hàm này là đạo hàm từng phần - đạo hàm 2 phía\n",
    "    \"\"\"\n",
    "    w_grad = np.zeros_like(w)\n",
    "    epsilon = 1e-3\n",
    "    for i in range(w.shape[0]):\n",
    "        for j in range(w.shape[1]):\n",
    "            w_right = w.copy()\n",
    "            w_left = w.copy()\n",
    "            w_right[i, j] += epsilon\n",
    "            w_left[i, j] -= epsilon\n",
    "            w_grad[i, j] = (cost(w_right) - cost(w_left)) / (2 * epsilon)\n",
    "    return w_grad\n",
    "\n",
    "def check_converged(theta, gradFunc, epsilon=1e-3):\n",
    "    \"\"\"\n",
    "    Kiểm tra sự hội tụ của đạo hàm tại điểm theta\n",
    "    \"\"\"\n",
    "    return np.linalg.norm(gradFunc(theta)) / len(theta) < epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BASIC CONCEPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GD(w_init, gradFunc, eta):\n",
    "    \"\"\"\n",
    "        w_init: điểm w khởi tạo, w.init là mảng 2 chiều\n",
    "        grad: hàm đạo hàm của hàm mục tiêu\n",
    "        eta: learning rate\n",
    "    \"\"\"\n",
    "    w = [w_init]\n",
    "    for ep in range(100):\n",
    "        w_next = w[-1] - eta * gradFunc(w[-1])\n",
    "        if check_converged(w_next, gradFunc):\n",
    "            break\n",
    "        w.append(w_next)\n",
    "    \n",
    "    return w, ep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WITH MOMENTUM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "v_t = \\gamma v_{t-1} + \\eta \\bigtriangledown_\\theta J (\\theta)\n",
    "$$\n",
    "$$\n",
    "\\theta = \\theta - v_t\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GD_momentum(theta_init, gradFunc, eta, gamma=0.9):\n",
    "    \"\"\"\n",
    "    theta_init: like w_init\n",
    "    eta: learning rate\n",
    "    gamma: hệ số - thường là 0.9 nhân với v trước\n",
    "    v_t = gamma * v_t-1 + eta * grad(theta)\n",
    "    \"\"\"\n",
    "    epochs = 5000\n",
    "    thetas = [theta_init]\n",
    "    v = np.zeros_like(theta_init)\n",
    "    for ep in range(epochs):\n",
    "        v_next = gamma * v + eta * gradFunc(thetas[-1])\n",
    "        theta_next = thetas[-1] - v_next\n",
    "        if check_converged(theta_next, gradFunc):\n",
    "            break\n",
    "        v = v_next\n",
    "        thetas.append(theta_next)\n",
    "        \n",
    "    return thetas, ep\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD - gradient right: w =  [[3.83628779]] \n",
      "6 iterations\n",
      "GD - gradient left: w =  [[-1.30585389]] \n",
      "34 iterations\n",
      "GD - numerical gradient: w =  [[-1.30635179]] \n",
      "31 iterations\n",
      "GD with momentum - numerical gradient: w =  [[-1.30431627]] \n",
      "243 iterations\n"
     ]
    }
   ],
   "source": [
    "theta_init = np.array([[5]]) # w phải có kích thước là (feature, 1)\n",
    "thetas, ep = GD(theta_init, grad, .1)\n",
    "print('GD - gradient right: w = ', thetas[-1].T, '\\n%d iterations' %(ep+1))\n",
    "thetas, ep = GD(theta_init, grad_left, .1)\n",
    "print('GD - gradient left: w = ', thetas[-1].T, '\\n%d iterations' %(ep+1))\n",
    "thetas, ep = GD(theta_init, numerical_grad, .1)\n",
    "print('GD - numerical gradient: w = ', thetas[-1].T, '\\n%d iterations' %(ep+1))\n",
    "thetas, ep = GD_momentum(theta_init, numerical_grad, .1)\n",
    "print('GD with momentum - numerical gradient: w = ', thetas[-1].T, '\\n%d iterations' %(ep+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NESTEROV ACCELERATED GRADIENT (NAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "v_t = \\gamma v_{t-1} + \\eta \\nabla_\\theta J (\\theta - \\gamma v_{t-1})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta = \\theta - v_t\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GD_nag(theta_init, gradFunc, eta, gamma=0.9):\n",
    "    epochs = 100\n",
    "    thetas = [theta_init]\n",
    "    v = np.zeros_like(theta_init)\n",
    "    for ep in range(epochs):\n",
    "        v_next = gamma * v + eta * gradFunc(thetas[-1] - gamma * v)\n",
    "        theta = thetas[-1] - v_next\n",
    "        if check_converged(thetas[-1], gradFunc, 1e-3):\n",
    "            break\n",
    "        thetas.append(theta)\n",
    "        v = v_next\n",
    "    return thetas, ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD - gradient right: w =  [[5]] \n",
      "1 iterations\n",
      "GD - gradient left: w =  [[-1.30599638]] \n",
      "21 iterations\n",
      "GD - numerical gradient: w =  [[-1.30649585]] \n",
      "21 iterations\n"
     ]
    }
   ],
   "source": [
    "theta_init = np.array([[5]]) # w phải có kích thước là (feature, 1)\n",
    "thetas, ep = GD_nag(theta_init, grad, .1)\n",
    "print('GD - gradient right: w = ', thetas[-1].T, '\\n%d iterations' %(ep+1))\n",
    "thetas, ep = GD_nag(theta_init, grad_left, .1)\n",
    "print('GD - gradient left: w = ', thetas[-1].T, '\\n%d iterations' %(ep+1))\n",
    "thetas, ep = GD_nag(theta_init, numerical_grad, .1)\n",
    "print('GD - numerical gradient: w = ', thetas[-1].T, '\\n%d iterations' %(ep+1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
