{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Có nhiều điều mình không thể hiểu hết về cách đạo hàm của Gradient Descent. <br>\n",
    "Có những bài đạo hàm phía trái, hoặc đạo hàm phía phải cho kết quả. <br>\n",
    "Hoặc là sử dụng Numerical Descent lại cho ra cực trị tối ưu - mặc dù không sử dụng momentum...(Mình có code bên dưới) <br>\n",
    "Bàn luận toán với mình về Gradient Descent <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's many things i can't understand all about how Gradient Descent work. <br>\n",
    "Some gradient by left, some gradient by right, or using Numerical Descent WITHOUT momentum give us optimal value... (I had code below) <br>\n",
    "You can help me to improve math in Gradient Descent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRADIENT DESCENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRADIENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(w: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Hàm mục tiêu để tìm minimum point\n",
    "    \"\"\"\n",
    "    return w ** 2 + 10 * np.sin(w)\n",
    "\n",
    "def grad(w: np.ndarray, cost) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Đạo hàm bên phải hàm cost \n",
    "    \"\"\"\n",
    "    w_grad = np.zeros_like(w)\n",
    "    epsilon = 1e-3\n",
    "    for i in range(w.shape[0]):\n",
    "        for j in range(w.shape[1]):\n",
    "            w_right = w.copy()\n",
    "            w_right[i, j] += epsilon\n",
    "            w_grad[i, j] = (cost(w_right) - cost(w)) / epsilon\n",
    "    return w_grad\n",
    "\n",
    "def grad_left(w: np.ndarray, cost) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Đạo hàm bên trái hàm cost \n",
    "    \"\"\"\n",
    "    w_grad = np.zeros_like(w)\n",
    "    epsilon = 1e-3\n",
    "    for i in range(w.shape[0]):\n",
    "        for j in range(w.shape[1]):\n",
    "            w_left = w.copy()\n",
    "            w_left[i, j] -= epsilon\n",
    "            w_grad[i, j] = (cost(w) - cost(w_left)) / epsilon\n",
    "    return w_grad\n",
    "\n",
    "def numerical_grad(w: np.ndarray, cost) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    w(right) - w(left) / 2 * epsilon\n",
    "    Đạo hàm này là đạo hàm từng phần - đạo hàm 2 phía\n",
    "    \"\"\"\n",
    "    w_grad = np.zeros_like(w)\n",
    "    epsilon = 1e-3\n",
    "    for i in range(w.shape[0]):\n",
    "        for j in range(w.shape[1]):\n",
    "            w_right = w.copy()\n",
    "            w_left = w.copy()\n",
    "            w_right[i, j] += epsilon\n",
    "            w_left[i, j] -= epsilon\n",
    "            w_grad[i, j] = (cost(w_right) - cost(w_left)) / (2 * epsilon)\n",
    "    return w_grad\n",
    "\n",
    "def check_converged(theta, gradFunc, cost, epsilon=1e-3):\n",
    "    \"\"\"\n",
    "    Kiểm tra sự hội tụ của đạo hàm tại điểm theta\n",
    "    \"\"\"\n",
    "    return np.linalg.norm(gradFunc(theta, cost)) / len(theta) < epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BASIC CONCEPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GD(w_init, gradFunc, cost, eta):\n",
    "    \"\"\"\n",
    "        w_init: điểm w khởi tạo, w.init là mảng 2 chiều\n",
    "        grad: hàm đạo hàm của hàm mục tiêu\n",
    "        eta: learning rate\n",
    "    \"\"\"\n",
    "    w = [w_init]\n",
    "    for ep in range(1000):\n",
    "        w_next = w[-1] - eta * gradFunc(w[-1], cost)\n",
    "        if check_converged(w_next, gradFunc, cost):\n",
    "            break\n",
    "        w.append(w_next)\n",
    "    \n",
    "    return w, ep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WITH MOMENTUM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "v_t = \\gamma v_{t-1} + \\eta \\bigtriangledown_\\theta J (\\theta)\n",
    "$$\n",
    "$$\n",
    "\\theta = \\theta - v_t\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GD_momentum(theta_init, gradFunc, cost, eta, gamma=0.9):\n",
    "    \"\"\"\n",
    "    theta_init: like w_init\n",
    "    eta: learning rate\n",
    "    gamma: hệ số - thường là 0.9 nhân với v trước\n",
    "    v_t = gamma * v_t-1 + eta * grad(theta)\n",
    "    \"\"\"\n",
    "    epochs = 5000\n",
    "    thetas = [theta_init]\n",
    "    v = np.zeros_like(theta_init)\n",
    "    for ep in range(epochs):\n",
    "        v_next = gamma * v + eta * gradFunc(thetas[-1], cost)\n",
    "        theta_next = thetas[-1] - v_next\n",
    "        if check_converged(theta_next, gradFunc, cost):\n",
    "            break\n",
    "        v = v_next\n",
    "        thetas.append(theta_next)\n",
    "        \n",
    "    return thetas, ep\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD - gradient right: w =  [[3.83628779]] \n",
      "6 iterations\n",
      "GD - gradient left: w =  [[-1.30585389]] \n",
      "34 iterations\n",
      "GD - numerical gradient: w =  [[-1.30635179]] \n",
      "31 iterations\n",
      "GD with momentum - numerical gradient: w =  [[-1.30431627]] \n",
      "243 iterations\n"
     ]
    }
   ],
   "source": [
    "theta_init = np.array([[5]]) # w phải có kích thước là (feature, 1)\n",
    "thetas, ep = GD(theta_init, grad, cost, .1)\n",
    "print('GD - gradient right: w = ', thetas[-1].T, '\\n%d iterations' %(ep+1))\n",
    "thetas, ep = GD(theta_init, grad_left, cost, .1)\n",
    "print('GD - gradient left: w = ', thetas[-1].T, '\\n%d iterations' %(ep+1))\n",
    "thetas, ep = GD(theta_init, numerical_grad, cost, .1)\n",
    "print('GD - numerical gradient: w = ', thetas[-1].T, '\\n%d iterations' %(ep+1))\n",
    "thetas, ep = GD_momentum(theta_init, numerical_grad, cost, .1)\n",
    "print('GD with momentum - numerical gradient: w = ', thetas[-1].T, '\\n%d iterations' %(ep+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NESTEROV ACCELERATED GRADIENT (NAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "v_t = \\gamma v_{t-1} + \\eta \\nabla_\\theta J (\\theta - \\gamma v_{t-1})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta = \\theta - v_t\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GD_nag(theta_init, gradFunc, cost, eta, gamma=0.9):\n",
    "    epochs = 100\n",
    "    thetas = [theta_init]\n",
    "    v = np.zeros_like(theta_init)\n",
    "    for ep in range(epochs):\n",
    "        v_next = gamma * v + eta * gradFunc(thetas[-1] - gamma * v, cost)\n",
    "        theta = thetas[-1] - v_next\n",
    "        if check_converged(thetas[-1], gradFunc, cost):\n",
    "            break\n",
    "        thetas.append(theta)\n",
    "        v = v_next\n",
    "    return thetas, ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD - gradient right: w =  [[5]] \n",
      "1 iterations\n",
      "GD - gradient left: w =  [[-1.30599638]] \n",
      "21 iterations\n",
      "GD - numerical gradient: w =  [[-1.30649585]] \n",
      "21 iterations\n"
     ]
    }
   ],
   "source": [
    "theta_init = np.array([[5]]) # w phải có kích thước là (feature, 1)\n",
    "thetas, ep = GD_nag(theta_init, grad, cost, .1)\n",
    "print('GD - gradient right: w = ', thetas[-1].T, '\\n%d iterations' %(ep+1))\n",
    "thetas, ep = GD_nag(theta_init, grad_left, cost, .1)\n",
    "print('GD - gradient left: w = ', thetas[-1].T, '\\n%d iterations' %(ep+1))\n",
    "thetas, ep = GD_nag(theta_init, numerical_grad, cost, .1)\n",
    "print('GD - numerical gradient: w = ', thetas[-1].T, '\\n%d iterations' %(ep+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STOCHASTIC GRADIENT DESCENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ưu: Thuật toán hội tụ rất nhanh!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ý tưởng chính của SGD là chọn điểm dữ liệu để đạo hàm. <br>\n",
    "Ở bài này, ta cho random ra hoán vị của N datapoint. Chẳng hạn như có 5 dữ liệu là [0, 1, 2, 3, 4], ta hoán vị random thì sẽ ra [3, 1, 2, 4, 0].<br>\n",
    "Từ đó ta chọn được điểm dữ liệu thứ i để đạo hàm (đọc hàm grad_single() để hiểu rõ hơn.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.rand(1000, 1) # random range(0, 1) with shape (1000, 1)\n",
    "y = 4 + 3 * X + .2 * np.random.randn(1000, 1)\n",
    "\n",
    "# f(x) = x * w^T (x.shape = (N, feature), w^T.shape = (feature, 1))\n",
    "ones = np.ones_like(X)\n",
    "Xbar = np.concatenate((ones, X), axis=1)\n",
    "N = len(Xbar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD - gradient: w =  [[3.99322352 2.92643375]] \n",
      "2 iterations\n",
      "GD NAG - gradient: w =  [[3.98374898 3.01617674]] \n",
      "72 iterations\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def lr_cost(w: np.ndarray, X=Xbar, Y=y) -> np.ndarray:\n",
    "    return .5 / X.shape[0] * np.linalg.norm(Y - X.dot(w), 2) ** 2\n",
    "    \n",
    "\n",
    "def grad_single(w: np.ndarray, i: int, permute: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Đạo hàm một điểm dữ liệu tại vị trí thứ true_i\n",
    "    Giả sử permute, i truyền vào lần lượt là [1, 3, 2, 0] và 1\n",
    "    => true_i = permute[1] = 3\n",
    "    \"\"\"\n",
    "    true_i = permute[i]\n",
    "    xi = Xbar[true_i, :]\n",
    "    yi = y[true_i]\n",
    "    epsilon = 1e-3\n",
    "    w_grad = np.zeros_like(w)\n",
    "    for i in range(w.shape[0]):\n",
    "        for j in range(w.shape[1]):\n",
    "            w_right = w.copy()\n",
    "            w_left = w.copy()\n",
    "            w_right[i, j] += epsilon\n",
    "            w_left[i, j] -= epsilon\n",
    "            w_grad[i, j] = (lr_cost(w_right, xi, yi) - lr_cost(w_left, xi, yi)) / (2 * epsilon)\n",
    "    return w_grad\n",
    "\n",
    "def SGD(w_init, eta):\n",
    "    \"\"\"\n",
    "    Đi qua giới hạn tối đa 10 epochs\n",
    "    Khi đó, mỗi epochs sẽ đi qua 1 lần lặp qua N điểm dữ liệu được shuffle mới\n",
    "    Cứ đi qua 10 điểm dữ liệu, thì w mới được cập nhật và kiểm tra hội tụ\n",
    "    Nếu chưa hội tụ thì w_toCheck là biến của 10 lần cập nhật w_next trước đó để kiểm tra\n",
    "    \"\"\"\n",
    "    w = [w_init]\n",
    "    epoch_check = 10\n",
    "    N = X.shape[0]\n",
    "    count = 0\n",
    "    temp_w = w_init\n",
    "    for ep in range(10):\n",
    "        permute = np.random.permutation(N)\n",
    "        for i in range(N):\n",
    "            count += 1\n",
    "            gradient = grad_single(w[-1], i, permute)\n",
    "            w_next = w[-1] - eta * gradient \n",
    "            w.append(w_next)\n",
    "            if count % epoch_check == 0:\n",
    "                if np.linalg.norm(w_next - temp_w, 2) / len(w_init) < 1e-3:                                    \n",
    "                    return w, ep\n",
    "                else: temp_w = w_next\n",
    "    return w, ep\n",
    "\n",
    "theta_init = np.array([[5.], [2.]])\n",
    "thetas, ep = SGD(theta_init, .1)\n",
    "print('GD - gradient: w = ', thetas[-1].T, '\\n%d iterations' %(ep+1))\n",
    "thetas, ep = GD_nag(theta_init, numerical_grad, lr_cost, .1)\n",
    "print('GD NAG - gradient: w = ', thetas[-1].T, '\\n%d iterations' %(ep+1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
